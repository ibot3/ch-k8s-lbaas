{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Start","text":"<p>Welcome to the documentation of the Cloud&amp;HEAT Load-Balancer-as-a-Service.</p>"},{"location":"#important-pages","title":"Important pages","text":"<ul> <li>Quickstart</li> <li>Components</li> <li>Config Options</li> </ul>"},{"location":"components/","title":"Components","text":"<p>LbaaS consists of two parts: The agent and the controller.</p>"},{"location":"components/#controller","title":"Controller","text":"<p>Maximum of one active controller at a time</p> <ul> <li>Kubernetes client<ul> <li>Watching for changes to services, nodes, endpoints, network policies</li> <li>Mapping new services of type <code>LoadBalancer</code></li> </ul> </li> <li>May run in k8s cluster or on gateway nodes (in cluster is easier!)</li> <li>Port Manager, maybe with OpenStack client</li> <li>Generates structured keepalived/nftables config requests and sends them to all configured agents</li> </ul>"},{"location":"components/#agent","title":"Agent","text":"<p>One agent on every gateway/load-balancer node</p> <ul> <li>HTTP endpoint for controller</li> <li>Generates nftables and keepalived config and applies the changes</li> </ul>"},{"location":"config/","title":"Config Options","text":"<p>Options with a <code>-</code> as default value are mandatory.</p>"},{"location":"config/#agent","title":"Agent","text":"Name Type Default Description shared-secret string - Secret that is shared with the controller(s) bind-address string - Bind IP address bind-port int - Bind TCP port keepalived Keepalived ... Keepalived configuration nftables Nftables ... Nftables configuration"},{"location":"config/#agent-keepalived","title":"Agent: Keepalived","text":"Name Type Default Description enabled bool true Enable keepalived config update vrrp-password string \"useless\" The VRRP password that is used, should be the same on all nodes priority int 0 The VRRP priority of the node virtual-router-id-base int - Virtual Router ID base interface string - Network interface used for VRRP service ServiceConfig ... Keepalived service configuration"},{"location":"config/#agent-nftables","title":"Agent: Nftables","text":"Name Type Default Description filter-table-name string \"filter\" Name of the nftables table for filtering rules filter-table-type string \"inet\" Type of the nftables table for filtering rules filter-forward-chain string \"forward\" Name of the nftables chain for filtering rules in the specified table nat-table-name string \"nat\" Name of the nftables table for NAT nat-prerouting-chain string \"prerouting\" Name of the nftables prerouting chain for NAT nat-postrouting-chain string \"postrouting\" Name of the nftables postrouting chain for NAT policy-prefix string \"\" Prefix for nftables chains created for k8s network policies; When partial-reload is enabled, all chains beginning with this prefix will be deleted on nftables config reload nft-command string list [\"sudo\", \"nft\"] Command to run <code>nft</code>; Required for partial-reload partial-reload bool false If partial-reload should be enabled; See Partial Reload; Causes lbaas-agent to load the last config on startup and include nft-commands to delete removed policy-chains in the generated config enable-snat bool true If SNAT should be enabled; Can be false if the load-balancer is also default gateway for the k8s nodes fwmark-bits uint 1 Mark that is used to mark load-balanced nftable/conntrack flows in the form: <code>mark 0x&lt;FWMarkBits&gt; and 0x&lt;FWMarkMask&gt;</code> fwmark-mask uint 1 See <code>FWMarkBits</code> service ServiceConfig ... Nftables service configuration"},{"location":"config/#agent-serviceconfig","title":"Agent: ServiceConfig","text":"Name Type Default Description config-file string - Path of the config file reload-command string list [\"sudo\", \"systemctl\", \"reload\", \"nftables\" or \"keepalived\"] Command to reload the service status-command string list [\"sudo\", \"systemctl\", \"is-active\", \"nftables\" or \"keepalived\"] Command to get status of the service, used for healthcheck after reload. If empty, the healthcheck is skipped start-command string list [\"sudo\", \"systemctl\", \"start\", \"nftables\" or \"keepalived\"] Command to start the service check-delay int 0 Delay (in seconds) between service reload and healthcheck"},{"location":"config/#controller","title":"Controller","text":"Name Type Default Description bind-address string - Bind IP address bind-port int 15203 Bind TCP port port-manager string \"openstack\" Port manager to use (\"openstack\" or \"static\") backend-layer string \"NodePort\" Backend layer to use openstack OpenStack ... OpenStack port manager configuration static Static ... Static port manager configuration agents Agents ... Agents configuration"},{"location":"config/#controller-openstack","title":"Controller: OpenStack","text":"Name Type Default Description auth Auth ... Auth configuration network Network ... Network configuration"},{"location":"config/#controller-openstack-auth","title":"Controller: OpenStack: Auth","text":"Name Type Default Description auth-url string - Keystone URL user-id string \"\" username string \"\" password string \"\" project-id string \"\" project-name string \"\" trust-id string \"\" domain-id string \"\" domain-name string \"\" project-domain-id string \"\" project-domain-name string \"\" user-domain-id string \"\" user-domain-name string \"\" region string - ca-file string \"\" application-credential-id string - application-credential-name string \"\" application-credential-secret string - tls-insecure bool false"},{"location":"config/#controller-openstack-network","title":"Controller: OpenStack: Network","text":"Name Type Default Description use-floating-ips bool false If floating-IPs should be used floating-ip-network-id string \"\" UUID of the floating-IP network subnet-id string \"\" UUID of the internal network"},{"location":"config/#controller-static","title":"Controller: Static","text":"Name Type Default Description ipv4-addresses string list [] List of IPv4 address that can be used for load-balancing"},{"location":"config/#controller-agents","title":"Controller: Agents","text":"Name Type Default Description shared-secret string - Shared secret with the agents token-lifetime int 15 Lifetime in seconds of the created JWT agents Agent list - List of agents"},{"location":"config/#controller-agents-agent","title":"Controller: Agents: Agent","text":"Name Type Default Description url string - URL to the agent HTTP endpoint"},{"location":"quickstart/","title":"Quickstart","text":"<p>Alternatively to the steps below, LBaaS can be deployed as part of yaook-k8s on OpenStack.</p>"},{"location":"quickstart/#requirements","title":"Requirements","text":"<ul> <li>A load-balancer node (e.g. Debian or VyOS) with nftables and (optional) keepalived </li> <li>A kubernetes cluster</li> <li>A network configuration on the load-balancer node that allows connections to th nodes/pods/service-addresses of the   kubernetes cluster</li> </ul>"},{"location":"quickstart/#setting-up-the-agent","title":"Setting up the agent","text":""},{"location":"quickstart/#getting-the-agent","title":"Getting the agent","text":""},{"location":"quickstart/#download-from-release","title":"Download from release","text":"<p>A built version of the agent can be found in the newest GitHub release.</p>"},{"location":"quickstart/#building-from-source","title":"Building from source","text":"<ol> <li>Clone the repository</li> <li>Run <code>m\u0300ake</code></li> </ol>"},{"location":"quickstart/#installing-the-agent","title":"Installing the agent","text":"<ol> <li>Copy <code>ch-k8s-lbaas-agent</code> to your load-balancer node</li> </ol>"},{"location":"quickstart/#running-the-agent","title":"Running the agent","text":"<ol> <li> <p>Create an agent config file, and save it somewhere (e.g. <code>/etc/ch-k8s-lbaas-agent/config.toml</code>). An example for     Debian can be found here.</p> </li> <li> <p>Start the agent: <code>./ch-k8s-lbaas-agent --config &lt;path-to-config&gt;</code></p> <ul> <li>Starting the agent as <code>root</code> is not recommended for production environments. It's recommended to create a separate     user and creating sudo rules that allow sudo usage for the required commands (e.g. <code>sudo nft</code>)</li> <li>It's recommended to create a systemd service for the agent</li> </ul> </li> </ol>"},{"location":"quickstart/#setting-up-the-controller-in-cluster","title":"Setting up the controller (in cluster)","text":""},{"location":"quickstart/#creating-a-configuration-secret","title":"Creating a configuration secret","text":"<p>Create a secret with the controller configuration by applying the following yaml file to the cluster:</p> <p>(example config with static port manager)</p> <pre><code>apiVersion: v1\nstringData:\n  controller-config.toml: |\n    port-manager=\"static\"\n    backend-layer=\"Pod\"\n\n    [static]\n    ipv4-addresses=[\"203.0.113.113\"]\n\n    [agents]\n    shared-secret=\"verysecure\"\n    token-lifetime=60\n\n    [[agents.agent]]\n    url=\"http://192.0.2.2:15203\"\nkind: Secret\nmetadata:\n  name: ch-k8s-lbaas-controller-config\n  namespace: kube-system\ntype: Opaque\n</code></pre>"},{"location":"quickstart/#creating-a-deployment","title":"Creating a deployment","text":"<p>The controller can be deployed by applying the following yaml file to the cluster:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ch-k8s-lbaas-controller\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ch-k8s-lbaas-controller\n  strategy:\n    rollingUpdate:\n      maxSurge: 0\n      maxUnavailable: 100%\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: ch-k8s-lbaas-controller\n    spec:\n      containers:\n      - args:\n        - --config\n        - /config/controller-config.toml\n        image: ghcr.io/cloudandheat/ch-k8s-lbaas/controller:0.5.0\n        name: controller\n        ports:\n        - containerPort: 15203\n          name: api\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /config\n          name: config\n          readOnly: true\n      nodeSelector:\n        node-role.kubernetes.io/master: \"\"\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n      volumes:\n      - name: config\n        secret:\n          defaultMode: 420\n          secretName: ch-k8s-lbaas-controller-config\n\n</code></pre>"},{"location":"agent/api/","title":"API","text":"<p>The agent offers an API to enable the controller to send an updated configuration.</p> <p>The following endpoints are available:</p> <ol> <li><code>POST /v1/apply</code><ul> <li>Content-Type: \"application/jwt\"</li> <li>JWT encoded JSON content encoded with shared-secret</li> <li>Python script for an example request can be found here </li> </ul> </li> </ol>"},{"location":"agent/keepalived/","title":"Keepalived","text":"<p>When keepalived is enabled in the configuration, it is used to make the load-balancer IP-addresses high-available by using VRRP between the load-balancer nodes.</p> <p>Each load-balancer IP-address is configured as virtual-address in keepalived. The available load-balancer node with the highest keepalived-priority will be selected as master and will configure the IP-addresses (as /32) on the given network interface</p>"},{"location":"agent/nftables/","title":"NFTables","text":"<p>The generation of the nftables config is one of the most important parts for making lbaas work. The nftables reference can be found here.</p> <p>This document describes which nftables tables and chains are used to insert the load-balancing rules.</p>"},{"location":"agent/nftables/#nat-table","title":"NAT Table","text":"<p>The NAT table specified in <code>nat-table-name</code> (config), is used to configure NAT rules.</p> <p></p> <p>The image shows an example scenario where DNAT and SNAT is used.</p>"},{"location":"agent/nftables/#destination-nat-nat-prerouting-chain","title":"Destination NAT (<code>nat-prerouting-chain</code>)","text":"<p>DNAT is used to do the actual load-balancing. An incoming packet is checked for it's destination address and port. If both match a load-balanced address and port, the destination address and port is NATed to the target service.</p> <p>As shown in the image above, the example service should be available on it's load-balancer IP-address <code>3.x.x.1</code> on port <code>80</code>. The load-balancer with nftables changes the destination of the packet to one of the available endpoints using the round-robin method. The new destination address is <code>10.x.x.2</code> with port <code>8080</code>.</p> <p>This is achieved by adding the following nftables rules in the <code>nat-prerouting-chain</code>:</p> <pre><code>ip daddr 3.x.x.1 tcp dport 80 meta mark set 0x00000001 ct mark set meta mark dnat to numgen inc mod 2 map { 0 : 10.x.x.1, 1 : 10.x.x.2 }:80\n</code></pre> <p>This lines does:</p> <ul> <li>Only apply for packets with matching TCP destination address and port (<code>ip daddr 3.x.x.1 tcp dport 80</code>)</li> <li>Set a mark for the nftables flow (<code>meta mark set 0x00000001</code>) also saves this mark in the conntrack information (<code>ct mark set meta mark</code>)</li> <li>Execute DNAT with an incrementing number generator modulo the number of targets pointing to a map of targets (<code>dnat to numgen inc mod 2 map { 0 : 10.x.x.1, 1 : 10.x.x.2 }:80</code>)   -&gt; Effectively, this is round-robin</li> </ul>"},{"location":"agent/nftables/#source-nat-nat-postrouting-chain","title":"Source NAT (<code>nat-postrouting-chain</code>)","text":"<p>When the load-balancer is also the default-gateway, the responses automatically come back to the load-balancer, where the DNAT is reversed. But if that is not the case, we also need to do SNAT, so that the load-balanced service endpoint sends the response to the load-balancer first, instead directly to the original sender.</p> <p>In the image above, this can be seen in the packet that the load-balancer sends to the load-balanced service. The source in this step changed from the original <code>5.x.x.1:2000</code> to <code>3.x.x.1:2000</code>.</p> <p>The corresponding nftables rule in the <code>nat-postrouting-chain</code> for this is:</p> <p><code>meta mark 0x00000001 masquerade</code></p> <p>The mark that we set in the DNAT/prerouting step is now used to enable masquerade SNAT for these packets.</p>"},{"location":"agent/nftables/#filter-table","title":"Filter Table","text":"<p>Rules and chains in the <code>filter-table-name</code> are used for two task:</p> <ol> <li>Allowing packet flows with the <code>0x00000001</code> mark (the flows that match a load-balancing rule)</li> <li>Enforcing network policies</li> </ol> <p>The entrypoint for all our rules is <code>filter-forward-chain</code>. In this chain we add two types of rules (in this order):</p> <ol> <li> <p><code>goto</code> rules for all network policies:</p> <p>Example: <code>ct mark 0x00000001 ip daddr 10.x.x.1 goto POD-10.x.x.1</code></p> <p>In this case, there is a network policy for the pod with address <code>10.x.x.1</code>. The contents of the policy are enforced in the chain with the name <code>POD-10.x.x.1</code>, that is also being created, together with some further (sub-)chains.</p> </li> <li> <p>A \"catch-all\" accept rule for all other packet flows that do not have a network-policy:</p> </li> </ol> <p><code>ct mark 0x00000001 accept</code></p> <p>In both rules, we check for the mark <code>ct mark 0x00000001 accept</code> to identify flows that belong to a load-balancing rule.</p>"},{"location":"agent/partial_reload/","title":"Partial Reload","text":"<p>The partial-reload feature for nftables can be used if there is no <code>nftables</code> service or similar that can just be reloaded.</p> <p>When the option is enabled, the behaviour of the agent changes as follows:</p> <ul> <li>The nftables config is reloaded on start of the agent, so that the last config is applied</li> <li>When generating the nftables config<ul> <li><code>flush chain</code> statements are rendered for <code>FilterForwardChainName</code>, <code>NATPreroutingChainName</code> and <code>NATPostroutingChainName</code></li> <li><code>delete chain</code> statements are rendered for all currently existing chains in the <code>FilterTableName</code> table starting with <code>PolicyPrefix</code></li> </ul> </li> </ul> <p>These changes allow lbaas to run in an environment where it isn't possible to reload the complete nftables ruleset. In this case, the changes can be applied with <code>nft -f</code>.</p>"},{"location":"agent/partial_reload/#example-config","title":"Example config","text":"<p>An example configuration for partial-reload usage on VyOS can be found here.</p>"},{"location":"agent/environments/debian/","title":"Debian","text":"<p>This page describes the requirements for running the LBaaS-agent on a Debian host.</p>"},{"location":"agent/environments/debian/#requirements","title":"Requirements","text":"<ul> <li>nftables and keepalived installed</li> <li>Network interface where load-balancer IP-addresses will be configured by keepalived</li> <li>L3-connection to the k8s-cluster<ul> <li>For example by establishing BGP peerings with the kubernetes nodes</li> </ul> </li> </ul>"},{"location":"agent/environments/debian/#prepare-nftables-config","title":"Prepare nftables config","text":"<p>The nftables config must be adjusted to create required tables/chains and include our custom nftables config file.  An example <code>/etc/nftables.conf</code> could look like this:</p> <p>Warning: This is only an example and might not be secure!</p> <pre><code>table inet filter {\n    chain input {\n        type filter hook input priority 0; policy accept;\n    }\n\n    chain forward {\n        type filter hook forward priority 0; policy accept;\n    }\n\n    chain output {\n        type filter hook output priority 0; policy accept;\n    }\n}\n\ntable ip nat {\n    chain postrouting {\n        type nat hook postrouting priority 100;\n    }\n\n    chain prerouting {\n        type nat hook prerouting priority 100;\n    }\n}\n\ninclude \"/var/lib/ch-k8s-lbaas-agent/nftables/*.conf\"\n</code></pre>"},{"location":"agent/environments/debian/#example-config","title":"Example config","text":"<p>As most of the default config values can be used in this case, the configuration file is very slim.</p> <pre><code>shared-secret=\"verysecure\"\nbind-address=\"0.0.0.0\"\nbind-port=15203\n\n[keepalived]\ninterface=\"ens3\"\npriority=100\nvirtual-router-id-base=10\n\n[keepalived.service]\nconfig-file=\"/var/lib/ch-k8s-lbaas-agent/keepalived/lbaas.conf\"\ncheck-delay=2\n\n[nftables.service]\nconfig-file=\"/var/lib/ch-k8s-lbaas-agent/nftables/lbaas.conf\"\n</code></pre>"},{"location":"agent/environments/vyos/","title":"VyOS","text":"<p>This page describes the requirements for running the LBaaS-agent on a VyOS host.</p>"},{"location":"agent/environments/vyos/#requirements","title":"Requirements","text":"<ul> <li>VyOS 1.3 (1.4 probably also works with some config changes)</li> <li>L3-connection to the k8s-cluster<ul> <li>For example by establishing BGP peerings with the kubernetes nodes (see below)</li> </ul> </li> <li>At least one configured SNAT or DNAT rule via the VyOS configuration interface <ul> <li>The created rule can also be disabled</li> <li>(Required because VyOS does not call the required NAT hooks if there are no NAT rules)</li> </ul> </li> <li>LBaaS-Controller with static port manager<ul> <li>Static IP-address(es) must be configured on the VyOS router</li> </ul> </li> </ul>"},{"location":"agent/environments/vyos/#notes","title":"Notes","text":"<ul> <li>Automatic keepalived configuration is not directly supported on VyOS. The high-availability configuration should be     configured independent of LBaaS.</li> </ul>"},{"location":"agent/environments/vyos/#example-config","title":"Example Config","text":""},{"location":"agent/environments/vyos/#vyos-13","title":"VyOS 1.3","text":"<pre><code>bind-address=\"0.0.0.0\"\nbind-port=15203\n\n[keepalived]\nenabled=false\n\n[nftables]\nfilter-table-name=\"filter\"\nfilter-table-type=\"ip\"\nfilter-forward-chain=\"VYATTA_PRE_FW_IN_HOOK\"\nnat-table-name=\"nat\"\nnat-prerouting-chain=\"VYATTA_PRE_DNAT_HOOK\"\nnat-postrouting-chain=\"VYATTA_PRE_SNAT_HOOK\"\npartial-reload=true\npolicy-prefix=\"lbaas-\"\nnft-command=[\"sudo\",\"nft\"]\nenable-snat=true\n\n[nftables.service]\nconfig-file=\"/var/lib/ch-k8s-lbaas-agent/nftables/lbaas.conf\"\nreload-command=[\"sudo\", \"nft\", \"-f\", \"/var/lib/ch-k8s-lbaas-agent/nftables/lbaas.conf\"]\nstatus-command=[\"true\"]\nstart-command=[\"sudo\", \"nft\", \"-f\", \"/var/lib/ch-k8s-lbaas-agent/nftables/lbaas.conf\"]\n</code></pre> <p>Warning: With VyOS 1.4, the names of the nftables hook changed.</p>"},{"location":"agent/environments/vyos/#bgp-configuration","title":"BGP Configuration","text":"<p>The LBaaS agent must be able to reach kubernetes-internal IP addresses like nodes, pods and services.</p> <p>One way to achieve this is using BGP. An example BGP config for VyOS could be created with this command:</p> <pre><code>set protocols bgp &lt;vyos-as&gt; neighbor &lt;k8s-node-ip&gt; remote-as &lt;k8s-as&gt;\n</code></pre> <p>The configuration should be applied for all nodes (<code>&lt;k8s-node-ip&gt;</code>), so that a peering is established with every  kubernetes node.</p> <p>For this to work, there must be some BGP routing daemon running on all nodes in the cluster. One option is to use Calico as CNI for kubernetes, which brings built-in BGP support (via bird). An example <code>BGPPeer</code> configuration for Calico could look like this:</p> <pre><code>apiVersion: crd.projectcalico.org/v1\nkind: BGPPeer\nmetadata:\n  name: vyos\nspec:\n  asNumber: &lt;vyos-as&gt;\n  keepOriginalNextHop: true\n  peerIP: &lt;vyos-ip&gt;\n</code></pre> <p>If multiple LBaaS agents should be used, multiple <code>BGPPeer</code> objects must be created.</p> <p>You can see if the configuration was successful by</p> <ol> <li>Checking if the BGP peerings are listed in <code>show bgp summary established</code></li> <li>Checking if the routes are present in <code>\u00ecp route list</code></li> </ol>"},{"location":"controller/backend_layer/","title":"Backend Layers","text":"<p>The backend-layer configuration option for the controller specified, which target should be used in the load-balancing process.</p> <p>There are currently three available backend layers:</p>"},{"location":"controller/backend_layer/#nodeport-default","title":"NodePort (default)","text":"<p>When using <code>NodePort</code> as backend layer, lbaas will balance the traffic to all nodes on the node port(s) specified in the k8s <code>LoadBalancer</code> service.</p>"},{"location":"controller/backend_layer/#clusterip","title":"ClusterIP","text":"<p>When using <code>ClusterIP</code> as backend layer, lbaas will forward the traffic to the cluster IP of the k8s <code>LoadBalancer</code> service. This implies that there is only one endpoint and the actual load-balancing between pods is done by the k8s cluster.</p>"},{"location":"controller/backend_layer/#pod","title":"Pod","text":"<p>When using <code>Pod</code> as backend layer, lbaas will register all pod IP-addresses that belong to the k8s <code>LoadBalancer</code> service  as endpoint for load-balancing. The k8s-internal load-balancer is not used.</p>"},{"location":"controller/k8s_client/","title":"Kubernetes Client","text":"<p>The k8s client in the controller is responsible for managing services and to receive events of changed cluster resources.</p>"},{"location":"controller/k8s_client/#events","title":"Events","text":"<p>The k8s client listens for the following changes in the cluster:</p> <ul> <li>Services (Add/Update/Delete)</li> </ul> <ul> <li>Triggers mapping/unmapping of corresponding services</li> <li>Triggers configuration update</li> </ul> <ul> <li>Nodes (Add/Update/Delete), if <code>NodePort</code> backend-layer is used</li> <li>Endpoints (Add/Update/Delete), if <code>Pod</code> backend-layer is used</li> <li>NetworkPolicies (Add/Update/Delete)</li> </ul> <ul> <li>Triggers configuration update</li> </ul>"},{"location":"controller/port_manager/","title":"Port Manager","text":"<p>The port manager is responsible for managing the load-balancer IP-addresses (also called L3-ports). </p>"},{"location":"controller/port_manager/#tasks","title":"Tasks","text":"<p>The port manager can do the following tasks:</p> <ul> <li>Provisioning new L3-ports</li> <li>Deleting unused L3-ports</li> <li>Returning a list of existing L3-ports</li> <li>Returning the external IP-address of an L3-port</li> <li>Returning the internal IP-address of an L3-port (may be the same as the external address)</li> <li>Checking if a given L3-port exists</li> </ul>"},{"location":"controller/port_manager/#implementations","title":"Implementations","text":"<p>There are currently two implementations for the port manager. The configuration can be done in the config file of the controller.</p>"},{"location":"controller/port_manager/#static","title":"Static","text":"<p>A simple implementation that just has a static list of IP-addresses that can be used for load-balancing.</p> <ul> <li>Provisioning new L3-ports or deleting unused ones is not possible.</li> <li>The ID of the L3-port is the load-balancer IP-address</li> <li>External and internal IP-addresses are the same (functions just return the given L3-port ID)</li> </ul>"},{"location":"controller/port_manager/#openstack","title":"OpenStack","text":"<p>A more complex implementation that can be used if the load-balancer gateways are running on OpenStack.</p> <ul> <li>Able to create new OpenStack ports with floating-IPs</li> <li>The ID of the L3-port is the OpenStack port ID (UUID)</li> <li>Unused L3-ports can be deleted using the cleanup function</li> <li>The external IP-address is the floating-IP, the internal IP-address is the internal address to which the floating-IP points to</li> </ul>"}]}